{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "transformer2017.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPWwIONPBSaU/HEYB/bY+wA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdityaKane2001/transformer2017/blob/main/transformer2017.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YR7EwdYdzB6S"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "MAX_SEQ_LEN = 64\n",
        "# BATCH_SIZE = 25000\n",
        "DROPOUT_RATE = 0.1\n",
        "EMBEDDING_DIMS = 512\n",
        "VOCABULARY_SIZE = 4096\n",
        "N_TRANSFORMERS = 6\n",
        "FFNN_DIMS = 2048\n",
        "NUM_HEADS = 8\n",
        "KEY_DIMS = EMBEDDING_DIMS/ NUM_HEADS\n",
        "VALUE_DIMS = EMBEDDING_DIMS/ NUM_HEADS\n",
        "\n",
        "\"\"\"\n",
        "Input pipeline:\n",
        "1. We get batch_size number of pairs of sentences from the dataset: \n",
        "    batch_size x  (\"My name is Aditya Kane\", \"<start> Ich bin Aditya Kane <end>\")\n",
        "2. These sentences are then tokenized: \n",
        "    batch_size x  ([2,3,4,5,6],[1,15,7,8,9,1000])\n",
        "3. The sentences are then padded to the largest sentence: \n",
        "    batch_size x ([2,3,4,5,6,0,0,0,0], [1,15,7,8,9,1000,0,0,0])\n",
        "4. They are then converted to embeddings:\n",
        "    batch_size x max_seq_len x embedding_dims\n",
        "5. Add positional embeddings to this\n",
        "    batch_size x max_seq_len x embedding_dims\n",
        "This is the input to our model.\n",
        "\"\"\"\n",
        "\n",
        "class PositionAwareEmbeddings(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dropout = layers.Dropout(DROPOUT_RATE)\n",
        "        self.embed_dims = EMBEDDING_DIMS\n",
        "        self.vocab_size = VOCABULARY_SIZE\n",
        "        self.embeddings = layers.Embedding(VOCABULARY_SIZE, EMBEDDING_DIMS,\n",
        "                                    input_length=MAX_SEQ_LEN)\n",
        "        self.max_seq_len = MAX_SEQ_LEN\n",
        "        \n",
        "\n",
        "    def get_positional_embeddings(self, input_seq_len):\n",
        "        positions = tf.reshape(tf.range(input_seq_len, dtype=tf.double), (input_seq_len,1))\n",
        "        freqs = tf.math.pow(10000, \n",
        "                -tf.range(0, self.embed_dims, delta=2) / self.embed_dims)\n",
        "\n",
        "        sin_embs = tf.transpose(tf.cast(tf.math.sin(positions * freqs), tf.float32))\n",
        "        cos_embs = tf.transpose(tf.cast(tf.math.cos( positions* freqs), tf.float32))\n",
        "        expanded_sin_embs = tf.scatter_nd( \n",
        "            indices = [[i] for i in range(512) if i%2==1],\n",
        "            updates = sin_embs,\n",
        "            shape = ( self.embed_dims, input_seq_len)\n",
        "        )\n",
        "        expanded_cos_embs = tf.scatter_nd( \n",
        "            indices = [[i] for i in range(512) if i%2==0],\n",
        "            updates = cos_embs,\n",
        "            shape = ( self.embed_dims, input_seq_len)\n",
        "        )\n",
        "        pos_embs = tf.transpose(expanded_sin_embs + expanded_cos_embs)\n",
        "        return pos_embs #, expanded_sin_embs,expanded_cos_embs\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_seq_len = inputs.shape[-1]\n",
        "        pos_emb = self.get_positional_embeddings(input_seq_len)\n",
        "        outputs = self.embeddings(inputs)\n",
        "        outputs += pos_emb\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class MultiheadAttention(layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.heads = NUM_HEADS\n",
        "\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pass\n",
        "\n",
        "class ResidualAddNormMHA(layers.layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        pass\n",
        "\n",
        "    def call(self, inputs):\n",
        "        pass"
      ],
      "metadata": {
        "id": "KDZCQPm-zJxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pe = PositionAwareEmbeddings()\n",
        "pos_embs, sin, cos = pe.get_positional_embeddings(100)\n",
        "# print(pos_embs.shape)"
      ],
      "metadata": {
        "id": "5MgnAYQn0PoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.constant([[[5, 5, 5, 5], [6, 6, 6, 6],\n",
        "                            [7, 7, 7, 7], [8, 8, 8, 8]],\n",
        "                           [[5, 5, 5, 5], [6, 6, 6, 6],\n",
        "                            [7, 7, 7, 7], [8, 8, 8, 8]]]).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdV_bCoSh8Su",
        "outputId": "102ac24e-70c9-4442-92d3-0ac395e8fd2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([2, 4, 4])"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "\n",
        "text_file = tf.keras.utils.get_file(\n",
        "    fname=\"spa-eng.zip\",\n",
        "    origin=\"http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "text_file = pathlib.Path(text_file).parent / \"spa-eng\" / \"spa.txt\""
      ],
      "metadata": {
        "id": "RNHFd7plmjw2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc2d7bc4-e56f-4c6a-f72b-68024646855d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n",
            "2654208/2638744 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(text_file) as f:\n",
        "    lines = f.read().split(\"\\n\")[:-1]\n",
        "text_pairs = []\n",
        "for line in lines:\n",
        "    eng, spa = line.split(\"\\t\")\n",
        "    spa = \"[start] \" + spa + \" [end]\"\n",
        "    text_pairs.append((eng, spa))"
      ],
      "metadata": {
        "id": "qoPL65kIKJ1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "for _ in range(5):\n",
        "    print(random.choice(text_pairs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8y53hLaoKW5y",
        "outputId": "6fd50fda-3a08-42c9-ed5e-42962a4a1608"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(\"We stayed up until about 2:30 studying for today's test.\", '[start] Permanecimos despiertos, hasta eso de las dos y media, estudiando para el examen de hoy. [end]')\n",
            "('The knife is dull.', '[start] El cuchillo está romo. [end]')\n",
            "('Beijing is bigger than Rome.', '[start] Pekín es más grande que Roma. [end]')\n",
            "(\"I'm looking for a bank. Is there one near here?\", '[start] Estoy buscando un banco. ¿Hay alguno por aquí cerca? [end]')\n",
            "('Stars can be seen at night.', '[start] Por la noche se pueden ver las estrellas. [end]')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(text_pairs\n",
        "    ))"
      ],
      "metadata": {
        "id": "Aq8IyRnDKaqr",
        "outputId": "9fc53b30-5fad-4c13-947f-87bf69c66a27",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "118964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5kwu8V0RKf4a"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}